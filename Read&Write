package main.scala

import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.elasticsearch.spark.sql._
import org.elasticsearch.spark._
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext._

import org.elasticsearch.spark.streaming._

object myElasticSearch {

  case class SimpsonCharacter(name: String, actor: String, episodeDebut: String)

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("Spark-Write-ElasticSearch").setMaster("local[*]")

    conf.set("es.index.auto.create", "true")

    val spark = SparkSession.builder()
      .appName("Spark-Write-ElasticSearch").master("local[*]")
      .config("es.index.auto.create", "true")
      .config("es.http.timeout", "5m")
      .config("es.nodes", "localhost")
      .config("es.port", "9205")
      .config("es.nodes.wan.only", "true")
      .getOrCreate()

    val index = "shows/data"

    import spark.implicits._

    val simpsonsDF = spark.sparkContext.parallelize(
      SimpsonCharacter("Homer", "Dan Castellaneta", "Good Night") ::
        SimpsonCharacter("Marge", "Julie Kavner", "Good Night") ::
        SimpsonCharacter("Bart", "Nancy Cartwright", "Good Night") ::
        SimpsonCharacter("Lisa", "Yeardley Smith", "Good Night") ::
        SimpsonCharacter("Maggie", "Liz Georges and more", "Good Night") ::
        SimpsonCharacter("Sideshow Bob", "Kelsey Grammer", "The Telltale Head") ::
        Nil).toDF().repartition(1)

    //writing to ElasticSearch index

    simpsonsDF.saveToEs(index)

    //reading from ElasticSearch index
    val df = spark.read.format("org.elasticsearch.spark.sql").option("es.resource", index).load()
     df.show()

    spark.stop;
  }
}
