package main.scala


import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._   
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming._
import org.elasticsearch.spark.streaming._
import org.elasticsearch.spark.streaming.EsSparkStreaming 
import scala.collection._

object myElasticSearch {

  case class SimpsonCharacter(name: String, actor: String, episodeDebut: String)

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("Spark-Write-ElasticSearch").setMaster("local[*]")

     conf.set("es.index.auto.create", "true")
     conf.set("es.index.auto.create", "true")
      conf.set("es.http.timeout", "5m")
      conf.set("es.nodes", "localhost")
      conf.set("es.port", "9205")
      conf.set("es.nodes.wan.only", "true")


val sc = new SparkContext(conf)                      
val ssc = new StreamingContext(sc, Seconds(1))       

val numbers = Map("one" -> 1, "two" -> 2, "three" -> 3)
val airports = Map("arrival" -> "delhi", "SFO" -> "San Jose")

val rdd = sc.makeRDD(Seq(numbers, airports))
val microbatches = mutable.Queue(rdd)                

ssc.queueStream(microbatches).saveToEs("spark/docs")

ssc.start()
ssc.awaitTermination() 

  }
 }
